<style>

.reveal .slides > sectionx {
    top: -70%;
}

.reveal pre code.r {background-color: #ccF}

.section .reveal li {color:white}
.section .reveal em {font-weight: bold; font-style: "none"}

</style>

```{r, echo=F}
head = function(...) knitr::kable(utils::head(...))
```



Analysing Texts and Networks with R
========================================================
author: Wouter van Atteveldt, Nel Ruigrok, Kasper Welbers
date: Session 2: Corpus and Network Analysis


Workshop Overview
===
type:section 

Session 1
+ Organizing & Transforming data
+ Accessing APIs from R

Session 2
+ *Corpus Analysis*
+ Network Analysis




Document-Term Matrix
===

+ Representation word frequencies
  + Rows: Documents
  + Columns: Terms (words)
  + Cells: Frequency
+ Stored as 'sparse' matrix
  + only non-zero values are stored
  + Usually, >99% of cells are zero
  
Docment-Term Matrix
===

```{r}
library(RTextTools)
m = create_matrix(c("I love data", "John loves data!"))
as.matrix(m)
```

Simple corpus analysis
===

```{r}
library(corpustools)
head(term.statistics(m))
```

Preprocessing 
===

+ Lot of noise in text:
  + Stop words (the, a, I, will)
  + Conjugations (love, loves)
  + Non-word terms (33$, !)
+ Simple preprocessing, e.g. in `RTextTools`
  + stemming
  + stop word removal

Linguistic Preprocessing
====

+ Lemmatizing
+ Part-of-Speech tagging
+ Coreference resolution
+ Disambiguation
+ Syntactic parsing  
+ Package `NLP`, `openNLP`
  + (requires `rJava`, pain to install)
  
Tokens
====

+ One word per line (CONLL)
+ Linguistic information 

```{r}
data(sotu)
head(sotu.tokens)
```

Getting tokens from AmCAT
===

```{r, eval=F}
tokens = amcat.gettokens(conn, project=1, articleset=set)
tokens = amcat.gettokens(conn, project=1, articleset=set, module="corenlp_lemmatize")
```

DTM from Tokens
===

```{r}
dtm = with(subset(sotu.tokens, pos1=="M"),
           dtm.create(aid, lemma))
dtm.wordcloud(dtm)
```

Corpus Statistics
===
```{r}
stats = term.statistics(dtm)
stats= arrange(stats, -termfreq)
head(stats)
```


Comparing Corpora
====
type:section

Compare speakers, media, periods, ...

Obama's speeches
====

```{r}
library(corpustools)
data(sotu)
obama = sotu.meta$id[sotu.meta$headline == "Barack Obama"]
dtm_o = with(subset(sotu.tokens, aid %in% obama & pos1 %in% c("N", "A", "M")),
           dtm.create(aid, lemma))
dtm.wordcloud(dtm_o)
```

Comparing Corpora
===
```{r}
dtm_b = with(subset(sotu.tokens, !(aid %in% obama) & pos1 %in% c("N", "A", "M")),
           dtm.create(aid, lemma))
cmp  = corpora.compare(dtm_o, dtm_b)
cmp = arrange(cmp, -chi)
head(cmp)
```

Contrast plots
===
```{r}
with(utils::head(cmp, 100),
plotWords(x=log(over), words = term, wordfreq = chi, random.y = T))
```

Comparing multiple corpora
====

+ Requires subcorpus variable
+ Align meta to dtm

```{r}
rownames(dtm_o)[1:3 ]
meta = sotu.meta[match(rownames(dtm_o), sotu.meta$id), ]
meta$year = format(meta$date, "%Y")
head(meta)
```

Comparing multiple corpora
===

```{r,message=F}
d = corpora.compare.list(dtm_o, as.character(meta$year), return.df=T, .progress="none")
d = arrange(d,-chi)
head(d)
```

Topic Modeling
====
type:section

Topic Modeling
====

+ Cluster words and documents
+ Comparable to factor analysis of DTM
+ Latent Dirichlet Allocation
  + Generative model
  + Writer picks mix of topics
    + Each topic is mix of words
  + Writer picks words from topics
+ Many advanced versions exist
  + Structural topic models
  + Hierarchical topic models
  
Topic Modeling in R
====

```{r}
library(corpustools)
set.seed(123)
m = lda.fit(dtm_o, K = 5, alpha = .1)
kable(terms(m, 10))
```

Visualizing LDA
====

```{r, eval=F}
library(LDAvis)
json = ldavis_json(m, dtm_o)
serVis(json)
```

How many topics? 
====

+ Inspect result
  + Mixed topics? increase K
  + Very similar topics? decrease K
+ Perplexity measure for different K 
  + Scree plot
+ Jacobi, Van Atteveldt & Welbers (2016)
+ (there will always be junk topics)

Perplexity
===

```{r}
p = readRDS("perplex.rds")
p = aggregate(p["p"], p["k"], mean)
library(ggplot2)
ggplot(p, aes(x=k, y=p, )) + geom_line()  +geom_point()
```

Analyzing LDA results
===

```{r}
tpd = topics.per.document(m)
tpd = merge(sotu.meta, tpd)
head(tpd)
```




Workshop Overview
===
type:section 

Session 1
+ Organizing & Transforming data
+ Accessing APIs from R

Session 2
+ Corpus Analysis
+ *Network Analysis*

(Social) Network analysis in R
===

+ Package `igraph`
+ Edges and Vertices
+ Set attrbiutes with `E(g)$label`, etc
+ Functions for clustering, centrality, plotting, etc.
+ Exporting/Importing to/from gephi, pajek, UCInet etc.

Semantic Network Analysis
===

+ Co-occurrence of concepts as semantic relation
+ Possibly limited to word-window
+ Useful to limit to e.g. nouns or noun+verbs
+ See e.g. Doerfel/Barnett 1999, Diesner 2013, Leydesdorff/Welbers 2011

Semantic Network Analysis in R
===

+ Package `semnet`
  + `github.com/kasperwelbers/semnet`
+ Input dtm or token list, output graph

```{r, eval=F}
library(semnet)
g = coOccurenceNetwork(dtm) 

g = windowedCoOccurenceNetwork(location, term, context)
```

Backbone extraction
===

+ Semantic networks are very large
+ Backbone extraction extracts most important edges

```{r, eval=F}
g_backbone = getBackboneNetwork(g, alpha=0.01, max.vertices=100)
```

Exporting graphs
===

+ Export to e.g. UCInet, gephi
+ More visualization, metrics

```{r, eval=F}
write.graph(g, filename, format)

library(rgexf)
gefx = igraph.to.gexf(g)
print(gefx, file="..")
```

Semnet for Sentiment Analysis
===

+ Sentiment around specific terms
+ Windowed co-occurrence of sentiment terms, concepts
+ More specific approach using syntax: Van Atteveldt et al., forthcoming

Hands-on session 2
===

Hand-outs: 
+ Corpus analysis
+ Comparing Corpora 
+ Topic Modeling
+ S/SN from Twitter

Thank You!
===
type: section

What you have learned:
+ Data management with R
+ Corpus Analysis
+ Topic Modeling
+ Social / Semantic Network Analysis

Go out and code!
